{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output, display\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_avisos():\n",
    "    url = \"https://www.yapo.cl/chile/comprar?ca=15_s&l=0&cmn=\"\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "    ultima_pagina_link = soup.find('span',{'class':'nohistory FloatRight'}).find('a').get('href')\n",
    "    ultima_pagina = int(ultima_pagina_link.split(\"o=\",1)[1])\n",
    "\n",
    "    inicio = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    #ultima_pagina = 2\n",
    "    total_data = []\n",
    "    variables = []\n",
    "    n = 0\n",
    "    \n",
    "    for i in range(1,ultima_pagina+1):\n",
    "        url_page = url + \"&st=a&o=\" + str(i)\n",
    "        #print(url_page)\n",
    "        data = requests.get(url_page)\n",
    "        soup_inner = BeautifulSoup(data.text, \"html.parser\")\n",
    "        \n",
    "        for row in soup_inner.find_all('tr', {'class':'ad listing_thumbs'}):\n",
    "            a = {}\n",
    "            #col = row.find_all('td')\n",
    "            a['data_scraping'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")    \n",
    "        \n",
    "            try: a['id_anuncio'] = row.get('id').encode(\"utf-8\") \n",
    "            except: variables.append('id_anuncio')\n",
    "            \n",
    "            try: a['enlace'] = str(row.find_all('td')[0].find('a').get('href').encode(\"utf-8\")).replace(\"b'\",\"\").replace(\"'\",\"\")\n",
    "            except: variables.append('enlace')\n",
    "            \n",
    "            try: a['titulo'] = row.find_all('td')[2].find('a').text.encode(\"utf-8\")\n",
    "            except: variables.append('titulo')\n",
    "            \n",
    "            try: a['precio'] = re.sub(r'(^[ \\t]+|[ \\t]+(?=:))', '', row.find_all('td')[2].find('span', {'class':'price'}).text, flags=re.M).replace(\"\\n\", \"\")\n",
    "            except: variables.append('precio')\n",
    "            \n",
    "            try: a['precio2'] = re.sub(r'(^[ \\t]+|[ \\t]+(?=:))', '', row.find_all('td')[2].find('span', {'class':'convertedPrice'}).text, flags=re.M).replace(\"\\n\", \"\")\n",
    "            except: variables.append('precio2')\n",
    "            \n",
    "            datos_iconos = row.find_all('td')[2]\n",
    "            \n",
    "            try: a['banos'] = str(datos_iconos).split('fa-bath',1)[1].split('</span>')[0].split('-text\">')[1]\n",
    "            except: variables.append('banos')\n",
    "            \n",
    "            try: a['dormitorios'] = str(datos_iconos).split('fa-bed',1)[1].split('</span>')[0].split('-text\">')[1]\n",
    "            except: variables.append('dormitorios')\n",
    "            \n",
    "            try: a['superficie'] = BeautifulSoup(str(datos_iconos).split('fa-expand',1)[1], \"html.parser\").text.replace(' icons__element-icon\">','').replace(\"\\n\", \"\")\n",
    "            except: variables.append('superficie')\n",
    "            \n",
    "            datos_ubicacion = row.find_all('td')[3]\n",
    "            \n",
    "            try: a['region'] = str(datos_ubicacion).split('class=\"region\">')[1].split('</span>')[0]\n",
    "            except: variables.append('region')\n",
    "            \n",
    "            try: a['comuna'] = str(datos_ubicacion).split('class=\"commune\">')[1].split('</span>')[0]\n",
    "            except: variables.append('comuna')\n",
    "            \n",
    "            try: a['company_ad'] = str(datos_ubicacion).split('title=\"Aviso profesional\">')[1].split('</span>')[0]\n",
    "            except: variables.append('company_ad')\n",
    "            \n",
    "            try: a['ubicacion'] = datos_ubicacion.find('i').get('class')[1]\n",
    "            except:variables.append('ubicacion')\n",
    "            \n",
    "            each_sales_request = requests.get(a['enlace'])\n",
    "            each_sales_soup = BeautifulSoup(each_sales_request.text, \"html.parser\")\n",
    "        \n",
    "            datos_aviso = each_sales_soup.find('section',{'class':'da-wrapper'})\n",
    "        \n",
    "            try: a['datetime_data'] = datos_aviso.find('article').find_all('div')[0].get('data-datetime')\n",
    "            except: variables.append('datetime_data')\n",
    "            \n",
    "            try: a['vendedor'] = datos_aviso.find('seller-info').get('username')\n",
    "            except: variables.append('vendedor')\n",
    "        \n",
    "            try: a['geo_data'] = str(datos_aviso.find('div',{'id':'modal_map_wrap'})).split(']')[0].replace('\"[','').split(' data-location=')[1].replace(\" \",\"\")\n",
    "            except: variables.append('geo_data')\n",
    "        \n",
    "            try: a['direccion'] = str(datos_aviso.find('h2')).replace('<h2>','').replace('</h2>','')\n",
    "            except: variables.append('direccion')\n",
    "        \n",
    "            try: a['imagen'] = datos_aviso.find('span',{'id':'display_image'}).find('img').get('src')\n",
    "            except: variables.append('imagen')\n",
    "            \n",
    "            try: a['descripcion'] = datos_aviso.find('p',{'itemprop':'description'}).text\n",
    "            except: variables.append('descripcion')\n",
    "        \n",
    "            try: \n",
    "                detalles_aviso = datos_aviso.find_all('div',{'class':'details'})\n",
    "                for tr in detalles_aviso:\n",
    "                    datos = tr.find_all('tr')\n",
    "                    for z in range(0,len(datos)):\n",
    "                        k = 0\n",
    "                        limit = len(datos)\n",
    "                        while k < limit: \n",
    "                            #print (str(datos[z].find('th')) + \" : \" + str(datos[z].find('td')))\n",
    "                            item = datos[z].find('th').text.encode('utf-8')\n",
    "                            item = re.sub(r\"b'\", \"\", str(item))\n",
    "                            item = re.sub(r\"'\", \"\", str(item))\n",
    "                            item = re.sub(r\"\\\\xc3\\\\xb3\", \"o\", str(item))\n",
    "                            if item not in variables:\n",
    "                                variables.append(item)\n",
    "                            try: itemvalue = datos[z].find('td').text.encode('utf-8')\n",
    "                            except: itemvalue = \"\"\n",
    "                            a[item] = itemvalue\n",
    "                            k += 1\n",
    "            except: pass\n",
    "                        \n",
    "            clear_output(wait=True)\n",
    "            n = n+1\n",
    "            print(\"Resultados encontrados! Cantidad de paginas a scrapear: \" + str(ultima_pagina))\n",
    "            print(\"Inicio: \" + inicio)\n",
    "            print(\"Scrapeando página: \" + url_page)\n",
    "            print(\"Avisos scrapeados: \" + str(n) + \" de \" + str(ultima_pagina*50) + \" aprox. (\" + str(n*100/(ultima_pagina*50)) + \")%\")\n",
    "            total_data.append(a)\n",
    "\n",
    "        \n",
    "    for i in range(0,len(total_data)):\n",
    "        for n in range(0,len(variables)):\n",
    "            if variables[n] not in total_data[i]:\n",
    "                total_data[i][variables[n]] = \"\"\n",
    "    \n",
    "    keys = total_data[0].keys()\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    csvname = \"data \"\n",
    "    with open(str(csvname)+str(now)+'.csv','w', newline='', encoding=\"utf-8-sig\", errors='surrogatepass') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(total_data)\n",
    "    print(\"Terminado!\")\n",
    "    print(\"Término: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\"))\n",
    "    print(\"Los resultados pueden ser encontrados en el archivo \"+str(csvname)+str(now)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados encontrados! Cantidad de paginas a scrapear: 997\n",
      "Inicio: 2021-05-02 14-05-37\n",
      "Scrapeando página: https://www.yapo.cl/chile/comprar?ca=15_s&l=0&cmn=&st=a&o=1\n",
      "Avisos scrapeados: 8 de 49850 aprox. (0.0160481444332999)%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-77fbacda91d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobtener_avisos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-693920c5fdc5>\u001b[0m in \u001b[0;36mobtener_avisos\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0meach_sales_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enlace'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0meach_sales_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach_sales_request\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mdatos_aviso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meach_sales_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'section'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'da-wrapper'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euras1a\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euras1a\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euras1a\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             warnings.warn(RuntimeWarning(\n",
      "\u001b[1;32mc:\\users\\euras1a\\appdata\\local\\programs\\python\\python36\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euras1a\\appdata\\local\\programs\\python\\python36\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mstartswith\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# < + letter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obtener_avisos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
