{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output, display\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_avisos():\n",
    "    url = \"https://www.yapo.cl/chile/comprar?ca=15_s&l=0&cmn=\"\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "    ultima_pagina_link = soup.find('span',{'class':'nohistory FloatRight'}).find('a').get('href')\n",
    "    ultima_pagina = int(ultima_pagina_link.split(\"o=\",1)[1])\n",
    "\n",
    "    inicio = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    #ultima_pagina = 2\n",
    "    total_data = []\n",
    "    variables = []\n",
    "    n = 0\n",
    "    \n",
    "    for i in range(1,ultima_pagina+1):\n",
    "        url_page = url + \"&st=a&o=\" + str(i)\n",
    "        #print(url_page)\n",
    "        data = requests.get(url_page)\n",
    "        soup_inner = BeautifulSoup(data.text, \"html.parser\")\n",
    "        \n",
    "        for row in soup_inner.find_all('tr', {'class':'ad listing_thumbs'}):\n",
    "            a = {}\n",
    "            #col = row.find_all('td')\n",
    "            a['data_scraping'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")    \n",
    "        \n",
    "            try: a['id_anuncio'] = row.get('id').encode(\"utf-8\") \n",
    "            except: variables.append('id_anuncio')\n",
    "            \n",
    "            try: a['enlace'] = str(row.find_all('td')[0].find('a').get('href').encode(\"utf-8\")).replace(\"b'\",\"\").replace(\"'\",\"\")\n",
    "            except: variables.append('enlace')\n",
    "            \n",
    "            try: a['titulo'] = row.find_all('td')[2].find('a').text.encode(\"utf-8\")\n",
    "            except: variables.append('titulo')\n",
    "            \n",
    "            try: a['precio'] = re.sub(r'(^[ \\t]+|[ \\t]+(?=:))', '', row.find_all('td')[2].find('span', {'class':'price'}).text, flags=re.M).replace(\"\\n\", \"\")\n",
    "            except: variables.append('precio')\n",
    "            \n",
    "            try: a['precio2'] = re.sub(r'(^[ \\t]+|[ \\t]+(?=:))', '', row.find_all('td')[2].find('span', {'class':'convertedPrice'}).text, flags=re.M).replace(\"\\n\", \"\")\n",
    "            except: variables.append('precio2')\n",
    "            \n",
    "            datos_iconos = row.find_all('td')[2]\n",
    "            \n",
    "            try: a['banos'] = str(datos_iconos).split('fa-bath',1)[1].split('</span>')[0].split('-text\">')[1]\n",
    "            except: variables.append('banos')\n",
    "            \n",
    "            try: a['dormitorios'] = str(datos_iconos).split('fa-bed',1)[1].split('</span>')[0].split('-text\">')[1]\n",
    "            except: variables.append('dormitorios')\n",
    "            \n",
    "            try: a['superficie'] = BeautifulSoup(str(datos_iconos).split('fa-expand',1)[1], \"html.parser\").text.replace(' icons__element-icon\">','').replace(\"\\n\", \"\")\n",
    "            except: variables.append('superficie')\n",
    "            \n",
    "            datos_ubicacion = row.find_all('td')[3]\n",
    "            \n",
    "            try: a['region'] = str(datos_ubicacion).split('class=\"region\">')[1].split('</span>')[0]\n",
    "            except: variables.append('region')\n",
    "            \n",
    "            try: a['comuna'] = str(datos_ubicacion).split('class=\"commune\">')[1].split('</span>')[0]\n",
    "            except: variables.append('comuna')\n",
    "            \n",
    "            try: a['company_ad'] = str(datos_ubicacion).split('title=\"Aviso profesional\">')[1].split('</span>')[0]\n",
    "            except: variables.append('company_ad')\n",
    "            \n",
    "            try: a['ubicacion'] = datos_ubicacion.find('i').get('class')[1]\n",
    "            except:variables.append('ubicacion')\n",
    "            \n",
    "            each_sales_request = requests.get(a['enlace'])\n",
    "            each_sales_soup = BeautifulSoup(each_sales_request.text, \"html.parser\")\n",
    "        \n",
    "            datos_aviso = each_sales_soup.find('section',{'class':'da-wrapper'})\n",
    "        \n",
    "            try: a['datetime_data'] = datos_aviso.find('article').find_all('div')[0].get('data-datetime')\n",
    "            except: variables.append('datetime_data')\n",
    "            \n",
    "            try: a['vendedor'] = datos_aviso.find('seller-info').get('username')\n",
    "            except: variables.append('vendedor')\n",
    "        \n",
    "            try: a['geo_data'] = str(datos_aviso.find('div',{'id':'modal_map_wrap'})).split(']')[0].replace('\"[','').split(' data-location=')[1].replace(\" \",\"\")\n",
    "            except: variables.append('geo_data')\n",
    "        \n",
    "            try: a['direccion'] = str(datos_aviso.find('h2')).replace('<h2>','').replace('</h2>','')\n",
    "            except: variables.append('direccion')\n",
    "        \n",
    "            try: a['imagen'] = datos_aviso.find('span',{'id':'display_image'}).find('img').get('src')\n",
    "            except: variables.append('imagen')\n",
    "            \n",
    "            try: a['descripcion'] = datos_aviso.find('p',{'itemprop':'description'}).text\n",
    "            except: variables.append('descripcion')\n",
    "        \n",
    "            try: \n",
    "                detalles_aviso = datos_aviso.find_all('div',{'class':'details'})\n",
    "                for tr in detalles_aviso:\n",
    "                    datos = tr.find_all('tr')\n",
    "                    for z in range(0,len(datos)):\n",
    "                        k = 0\n",
    "                        limit = len(datos)\n",
    "                        while k < limit: \n",
    "                            #print (str(datos[z].find('th')) + \" : \" + str(datos[z].find('td')))\n",
    "                            item = datos[z].find('th').text.encode('utf-8')\n",
    "                            item = re.sub(r\"b'\", \"\", str(item))\n",
    "                            item = re.sub(r\"'\", \"\", str(item))\n",
    "                            item = re.sub(r\"\\\\xc3\\\\xb3\", \"o\", str(item))\n",
    "                            if item not in variables:\n",
    "                                variables.append(item)\n",
    "                            try: itemvalue = datos[z].find('td').text.encode('utf-8')\n",
    "                            except: itemvalue = \"\"\n",
    "                            a[item] = itemvalue\n",
    "                            k += 1\n",
    "            except: pass\n",
    "                        \n",
    "            clear_output(wait=True)\n",
    "            n = n+1\n",
    "            print(\"Resultados encontrados! Cantidad de paginas a scrapear: \" + str(ultima_pagina))\n",
    "            print(\"Inicio: \" + inicio)\n",
    "            print(\"Scrapeando página: \" + url_page)\n",
    "            print(\"Avisos scrapeados: \" + str(n) + \" de \" + str(ultima_pagina*50) + \" aprox. (\" + str(n*100/(ultima_pagina*50)) + \")%\")\n",
    "            total_data.append(a)\n",
    "\n",
    "        \n",
    "    for i in range(0,len(total_data)):\n",
    "        for n in range(0,len(variables)):\n",
    "            if variables[n] not in total_data[i]:\n",
    "                total_data[i][variables[n]] = \"\"\n",
    "    \n",
    "    keys = total_data[0].keys()\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    csvname = \"data \"\n",
    "    with open(str(csvname)+str(now)+'.csv','w', newline='', encoding=\"utf-8-sig\", errors='surrogatepass') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(total_data)\n",
    "    print(\"Terminado!\")\n",
    "    print(\"Término: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\"))\n",
    "    print(\"Los resultados pueden ser encontrados en el archivo \"+str(csvname)+str(now)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtener_avisos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
